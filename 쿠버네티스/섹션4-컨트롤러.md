## 컨트롤러

### Auto Healing

<img width="273" alt="image" src="https://github.com/yujindonut/Study/assets/78431728/1cb89d1b-fec0-40d1-a8d1-3aa73749214c">

특정 노드가 죽거나, 특정 노드 안의 pod이 죽었을때, Controller는 이를 체크하여 즉각적으로 다른 node에 해당 pod을 생성해준다.

### Auto scaling

Pod에 resource 상태가 limit 상태가 되었을때, pod를 또 생성해줘서 트래픽이 넘치지 않도록 도와준다.

### Software Update

update 도 controller를 통해서 한번에 쉽게 가능, 문제가 생기면 롤백도 쉽게 진행이 가능하다.

### job
일시적인 작업을 진행해야할 경우, controller가 필요한 경우에만 pod을 생성하고 다시 자원을 반환한다.
효율적인 자원이용이 가능하다.


------------------------------------------------------------------------------------------------

## Replication Controller, Replica Set
Replication Controller - deprecated 됨

![image](https://github.com/yujindonut/Study/assets/78431728/75973113-01ab-43f4-b8c8-aca7e2ecb43e)

- Template
  label 을 지정해둬야 template이랑 연결이 된다.
  특정 pod가 죽게 되면, replication안의 template대로 pod을 생성한다.
  해당 template이 Update 되면, 만들어놨던 v1의 pod들은 없애주고, 다시 생성한다. 

- Replicas
  자동으로 scale out, scale in 이 가능해진다.

```
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    type: web
spec:
  containers:
  - name: container
    image: kubetm/app:v1
  terminationGracePeriodSeconds: 0
```

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1 // 해당 replica 개수를 유지하려 하기 때문에, pod이 삭제되어도 새로 생성됨.
  selector:
    matchLabels:
      type: web
  template:
    metadata:
      name: pod1 
      labels:
        type: web
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

ReplicationController

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: replication1
spec:
  replicas: 2
  selector:
    cascade: "false"
  template:
    metadata:
      labels:
        cascade: "false"
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
```

replication controller를 삭제하면 pod도 삭제되기때문에 해당 명령어를 넣으면 controller만 삭제된다.
```
kubectl delete replicationcontrollers replication1 --cascade=false
```

ReplicaSet
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica2
spec:
  replicas: 2
  selector:
    matchLabels:
      cascade: "false"
  template:
    metadata:
      labels:
        cascade: "false"
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
```

#### Selector

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 1
  selector:
    matchLabels:
      type: web
      ver: v1  // 라벨이랑 밑의 template.metadata.lables과 매칭이 안되면 생성이 되지 않는다.
    matchExpressions:
    - {key: type, operator: In, values: [web]}
    - {key: ver, operator: Exists}
  template:
    metadata:
      labels:
        type: web
        ver: v1
        ver: v2
        location: dev
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

----------------------------------------------------------------------------------------------------

# Deployment

한 서비스에서 오류가 나서 재배포 또는 템플릿에 update가 생겼을 때 사용됨
![image](https://github.com/yujindonut/Study/assets/78431728/671f2966-7261-4e94-957e-e90c1507ea89)

### Recreate
- downtime 발생
- 추가적인 팟 필요 없음

### Rolling Update
- pod의 트래픽을 점차적으로 v2버전으로 옮겨감
- 새로운 template이 생기면서 rollung update가 실행됨
- v1의 replica set 트래픽 수가 2->1->0
- v2의 replica set 트래픽 수가 0->1->2

### Blue/Green
- controller를 하나 더 만든다
- Service 의 라벨만 변경하면 롤백이 쉽다
- 팟이 각 버전의 두배가 필요하다.

### Canary
- 일산화 산소 위험을 빨리 알아차리는 카나리아 새를 이용한 이름
- ty에 붙어진 이름을 통해서, v1, v2의 모든 버전을 배포 함.
- ingress controller 를 이용해서 해당 url에 접근하는 사람만 canary팟에 보낼 수 있음
- zero down time

### 실습

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  selector:
    matchLabels:
      type: app
  replicas: 2
  strategy:
    type: Recreate
  revisionHistoryLimit: 1
  template:
    metadata:
      labels:
        type: app
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 10
```

```
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    type: app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080

```

```
while true; do curl 10.99.5.3:8080/version; sleep 1; done

직접 커맨드로 deployment 방식으로 변경해야할때,
kubectl rollout undo deployment deployment-1 --to-revision=2
kubectl rollout history deployment deployment-1
```

### Rolling Update
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-2
spec:
  selector:
    matchLabels:
      type: app2
  replicas: 2
  strategy:
    type: RollingUpdate
  minReadySeconds: 10
  template:
    metadata:
      labels:
        type: app2
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

```
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    type: app2
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
```

```
while true; do curl 10.99.5.3:8080/version; sleep 1; done
```

### Blue/Green

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica1
spec:
  replicas: 2
  selector:
    matchLabels:
      ver: v1
  template:
    metadata:
      name: pod1
      labels:
        ver: v1
    spec:
      containers:
      - name: container
        image: kubetm/app:v1
      terminationGracePeriodSeconds: 0
```

```
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    ver: v1
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
```



--------------------------------------------------------------------------------------------------------

### DaemonSet, Job, CronJob

![image](https://github.com/yujindonut/Study/assets/78431728/dfea2d02-9705-4ca3-9a92-f980ce86ffa1)


#### Daemonset

![image](https://github.com/yujindonut/Study/assets/78431728/9399c334-65ae-4e7a-a2fb-aa3b952b306c)
특정 노드 또는 모든 노드에 항상 실행되어야할 특정 파드를 관리한다.

#### Job

#### Cron Job


## 실습

### DaemonSet - HostPort

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-1 // 데몬셋의 이름
spec:
  selector:
    matchLabels:
      type: app // 어떤 레이블의 파드를 선택하여 관리할지 설정
  template:
    metadata:
      labels:
        type: app // 생성할 파드의 레이블 타입
    spec:
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
          hostPort: 18080 // 이 port로 접속하면 노드 ip:hostport 하면 모든 pod에 접속이 가능하다.
```

### DaemonSet - NodeSelector
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-2
spec:
  selector:
    matchLabels:
      type: app
  template:
    metadata:
      labels:
        type: app
    spec:
      nodeSelector:
        os: centos
      containers:
      - name: container
        image: kubetm/app
        ports:
        - containerPort: 8080
```

```
kubectl label nodes k8s-node1 os=centos // os라벨을 다는 것
kubectl label nodes k8s-node2 os=ubuntu
```

데몬셋을 삭제하면 데몬셋이 생성한 pod들이 삭제됨
default는 rolling update가 된다.

-----------------------------------------------------------
### Job

```
apiVersion: batch/v1
kind: Job
metadata:
  name: job-1
spec:
  template:
    spec:
      restartPolicy: Never # 파드의 재시작 정책 설정 (
      containers:
      - name: container
        image: kubetm/init
        command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
      terminationGracePeriodSeconds: 0 
```
![image](https://github.com/yujindonut/Study/assets/78431728/3f9a3959-0662-4f4a-8f44-45b6658f3cc0)

```
apiVersion: batch/v1
kind: Job
metadata:
  name: job-2
spec:
  completions: 6 // 6개의 pod를 준비시킴
  parallelism: 2 // 병렬 2개씩
  activeDeadlineSeconds: 30 초가 넘어가면 job은 중지가 되고, 이전 job을 중지시킨다.
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: container
        image: kubetm/init
        command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
      terminationGracePeriodSeconds: 0
```

restartPolicy : {Never: 재시작하지 않는다. | onFailure : 비정상 종료 발생 시 컨테이너를 재시작 | always : 항상 재시작한다. }
activeDeadlineSeconds : 파드의 실행 시간을 지정한다. 파드가 이보다 오래 실행되면 시스템은 파드를 종료하려고 시도하고 잡을 실패한 것으로 표시한다.

20초가 넘으면 기존의 pod들은 terminate 되고 다른 job들은 삭제됨. 방금 새로 만든 pod들은 30초가 넘어가면, 지정된 시간보다 활성화되었다는 표시와 함께 job들이 멈춘다.

### Cron Job

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cron-job
spec:
  schedule: "*/1 * * * *" // 1분에 하나씩 만든다.
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 20; echo 'job end'"]
          terminationGracePeriodSeconds: 0
```

```
kubectl create job --from=cronjob/cron-job cron-job-manual-001
kubectl patch cronjobs cron-job -p '{"spec" : {"suspend" : false }}' // patch 명령어로 Job을 실행시킬 수 있음
```


### CronJob - Concurrency Policy

![image](https://github.com/yujindonut/Study/assets/78431728/0353009e-a1ff-4a4a-8da8-28fc33e2bee4)

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cron-job-2
spec:
  schedule: "20,21,22 * * * *"
  concurrencyPolicy: Replace // 앞에 만들어진 job이 replace 됨
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 140; echo 'job end'"]
          terminationGracePeriodSeconds: 0

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cron-job-2
spec:
  schedule: "20,21,22 * * * *"
  concurrencyPolicy: Replace // 앞에 만들어진 job이 replace 됨
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: container
            image: kubetm/init
            command: ["sh", "-c", "echo 'job start';sleep 140; echo 'job end'"]
          terminationGracePeriodSeconds: 0
```
